{
  "comments": [
    {
      "key": {
        "uuid": "5a93c517_7e565208",
        "filename": "k8s/k8sclient/k8sclient.py",
        "patchSetId": 2
      },
      "lineNbr": 286,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-22T22:11:35Z",
      "side": 1,
      "message": "Looks to me that you will attempt to do a clean up automatically upon an exception.  This is a different approach from the docker plugin (and possibly the cdap plugin) where failures were simply reported.  The idea was to give operators the opportunity to investigate.  Clean up was a manual second step.  Should we change our approach?",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_47ace68c",
        "filename": "k8s/k8sclient/k8sclient.py",
        "patchSetId": 2
      },
      "lineNbr": 286,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T12:13:25Z",
      "side": 1,
      "message": "I think the difference is that the Docker plugin essentially attempted to create and deploy one thing--a container.  k8sclient creates multiple k8s entries (none of which are known to the caller) as part of the deploy operation. The caller is not aware of the different k8s entities deployed by k8sclient, so it would not know how to do a cleanup.",
      "parentUuid": "5a93c517_7e565208",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_0a0583c1",
        "filename": "k8s/k8sclient/k8sclient.py",
        "patchSetId": 2
      },
      "lineNbr": 286,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-23T14:15:09Z",
      "side": 1,
      "message": "The caller wouldn\u0027t have to know about the intricacies of the clean up right?  The caller would simply be expected to call the \"undeploy\" function below which seems to call the same functions here.\n\nGiven that I want to make sure that for debugging deployment issues, will having these particular k8 entities not be useful to have lying around for investigation purposes until a manual undeploy?",
      "parentUuid": "5a93c517_47ace68c",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_0a04a37c",
        "filename": "k8s/k8sclient/k8sclient.py",
        "patchSetId": 2
      },
      "lineNbr": 286,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T14:55:47Z",
      "side": 1,
      "message": "I don\u0027t think having successfully deployed entities left around will help much with debugging.  Yes, in theory it\u0027s possible, but in practice, with Kubernetes, I doubt it.  There is a huge downside, though, to leaving orphaned components around in a production setting.\n\nk8sclient does not maintain state about what it has deployed after the deploy() function finishes.  After a successful deployment, it passes back a handle that the caller can store into runtime_properties.  This can be used for an undeploy() operation during a later uninstall workflow.   If deployment fails, k8sclient throws an exception, and it can\u0027t pass back the handle. \n\nSo to do what you\u0027re suggesting is more than just ripping out the code that does the cleanup--it requires maintaining some state in k8sclient after deploy() exits, whether through an exception or a normal return.   \n\nOn a normal exit, deploy() will still have to return the handle, so that it can stored in runtime_properties for the normal case: when undeploy() is invoked as a separate Cloudify lifecycle operation.  \n\nUndeploy() will need two separate modes of operation--one during the install workflow, when the caller will not have a handle, and one during the uninstall workflow, when k8sclient cannot possibly have any state and will need to get the handle from the caller.  Or maybe there\u0027s a separate cleanup() function that\u0027s used only for the install workflow case.\n\nI do not think there is a compelling case for the change you want.  If you insist, I can do it.  It will take a couple of days--mostly to do all the necessary testing.  Given that, it would be better to do this as bug post-M4. (The bug is that the requirement to handle errors in the way you suggest was not explicitly stated post-M4.)",
      "parentUuid": "5a93c517_0a0583c1",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_28f89048",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 4,
      "author": {
        "id": 515
      },
      "writtenOn": "2018-03-22T19:17:29Z",
      "side": 1,
      "message": "2018?",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_07b6dedc",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 4,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T12:28:16Z",
      "side": 1,
      "message": "2017-2018",
      "parentUuid": "5a93c517_28f89048",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_5e64b6e4",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 41,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-22T22:11:35Z",
      "side": 1,
      "message": "Just want to point out that evaluating \"configure()\" here makes unit testing a bit trickier and impacts all unit tests that fall under tasks.  An alt approach is to do a lazy evaluation by invoking getter type functions when configuration parameters are needed.  Not a blocker because I see that you have the \"mockconfig\" fixture.",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_e7b412d3",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 41,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T12:28:16Z",
      "side": 1,
      "message": "Yep, unit test is a hassle with this.   Will look at the lazy evaluation approach in a later release.",
      "parentUuid": "5a93c517_5e64b6e4",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_9ea9dee1",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 324,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-22T22:11:35Z",
      "side": 1,
      "message": "Want to point out that doing this copy shouldn\u0027t be necessary because it should have been done by the \"merge_inputs_for_*\" decorators.",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_1e470e4d",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 453,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-22T22:11:35Z",
      "side": 1,
      "message": "The data router subscriber case may potentially need to be updated.  I\u0027m not sure if it will be the case that components will register with Consul going forward.  If they will not then the \"_lookup_service\" call will need to change to get the ip or hostname from somewhere else.",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_47c38638",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 453,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T12:28:16Z",
      "side": 1,
      "message": "Agree.  We don\u0027t have a full DMaaP with DR in the R2 use cases.",
      "parentUuid": "5a93c517_1e470e4d",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_fe6022d1",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 615,
      "author": {
        "id": 1071
      },
      "writtenOn": "2018-03-22T22:11:35Z",
      "side": 1,
      "message": "With the removal of the \"SelectedDockerHost\" node types, we have taken a step back in terms of multi-site support and will need to come up with an answer for this.  Not sure if the plan will be to deploy a cluster per site and thus we will need a cluster selector.",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_67d7a2ee",
        "filename": "k8s/k8splugin/tasks.py",
        "patchSetId": 2
      },
      "lineNbr": 615,
      "author": {
        "id": 511
      },
      "writtenOn": "2018-03-23T12:28:16Z",
      "side": 1,
      "message": "SelectedDockerHost is not something that would work in the Kubernetes world.  (If you really want to, you can force a pod to be scheduled on a particular node.  The normal case is to let Kubernetes do its job as a scheduler.   Even if we implemented this capability, it wouldn\u0027t help with multi-site, because it\u0027s not possible to schedule across clusters.)  \n\nI thought about something analogous like \"KubernetesCluster\" node and a \"hosted_on\" type of relationship, but until we have a clearer understanding of how we\u0027re going to do multiple sites (and how we model and manage locations generally), doing so would just be speculative.  (If we just have a bunch of separate clusters, we\u0027d take one approach.  If we use the Kubernetes federation work, we\u0027d need a different approach.)",
      "parentUuid": "5a93c517_fe6022d1",
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_a3d5a3ba",
        "filename": "k8s/pom.xml",
        "patchSetId": 2
      },
      "lineNbr": 4,
      "author": {
        "id": 1949
      },
      "writtenOn": "2018-03-22T20:41:15Z",
      "side": 1,
      "message": "the same as in other comment: 2017-2018",
      "range": {
        "startLine": 4,
        "startChar": 14,
        "endLine": 4,
        "endChar": 18
      },
      "revId": "18725870db72390527466d2fe4d4107adb7100b3",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    }
  ]
}